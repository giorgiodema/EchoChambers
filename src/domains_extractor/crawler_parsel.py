
import itertools
import os
import pickle
import queue
import requests
import threading
from parsel import Selector
from pprint import pprint
from functools import reduce




# - - - - - - -    CRAWLING FUNCTIONS    - - - - - -

REQUEST_TIMEOUT = 5

def extract_links(url: str) -> list:
	""" Takes "url" returns list of all links of that url """
	try:
		text = requests.get(url, timeout=REQUEST_TIMEOUT).text
	except: 
		return []

	sel = Selector(text=text)
	links = sel.xpath('//a/@href').getall()
	print(f'[{threading.get_ident()}] Extracted', len(links), 'links from', url)
	return links


def extract_domains(links: list) -> set:
	""" given a list of links extract unique domains"""
	domains = set(map(extract_domain, links))
	# stats
	print('\n\nTotal links', len(links))
	print('Total unique domains', len(domains))
	return domains

def extract_domain(link: str) -> str:
	""" Extract the domain of a single link, returns None if domain does not start with http(s) or www """
	link_split = link.split('/')
	if link.startswith('http'):
		domain = link_split[2] if len(link_split)>=3 else None
	elif link.startswith('www'):
		domain = link_split[0]
	else:
		domain = None
	return domain








# - - - - - - -    MULTITHREADING FUNCTIONS    - - - - - - - - - -


WORKER_TIMEOUT = REQUEST_TIMEOUT + 5   #request timeout + some processing time (exagerated)

def multiworker_extract(base_url: str, levels: int, workers: int):
	""" Returns a tuple (unique domains, all visited links) by visiting 'base_url' 
	with 'levels' recurrence with 'workers' parallel jobs"""

	# Use one queue per recursive levels, each queue is a list of links.
	# queues[i] contains links to be recursively crawled 'i-1' times.
	# queues[0] will contain all the visited links
	queues = [queue.Queue() for l in range(levels+1)]
	queues[levels].put(base_url)

	threads = [threading.Thread(target=worker_procedure, args=(queues, )) for i in range(workers)]
	for p in threads:
		p.start()

	all_links = []
	result_queue = queues[0]
	while True:
		try:
			all_links.append(result_queue.get(True, 2*WORKER_TIMEOUT))
		except queue.Empty:
			break

	for p, i in zip(threads, range(len(threads))):
		p.join()
		print('[+] Joined process', i, 'of', len(threads))

	return extract_domains(all_links), all_links


def link_iterator(queues: list):
	""" Iterator returning the next url to crawl"""
	for i in reversed(range(1, len(queues))):	#discard queues[0]: links there have already been crawled
		while True:
			try:
				url = queues[i].get(True, WORKER_TIMEOUT)
				yield i, url   
			except queue.Empty:
				print(f'[{threading.get_ident()}] Exhausted queue', i)
				break


def worker_procedure(queues: list):
	iterator = link_iterator(queues)
	for recurr_lvl, url in iterator:
		extr_links = extract_links(url)
		# push extracted links in the next queue
		next_queue = queues[recurr_lvl-1]
		for extr_link in extr_links:
			next_queue.put(extr_link)
		# push current url in the result queue, aka queues[0]
		queues[0].put(url)

	print(f'[{threading.get_ident()}] Stopping thread...')










# - - - - - -    MAIN    - - - - - - -

if __name__ == '__main__':
	base_url = 'https://www.bufale.net/the-black-list-la-lista-nera-del-web/'
	#base_links = extract_links(base_url)
	#domains1, links1 = multiprocess_extract(base_links, 1)
	domains, links = multiworker_extract(base_url, 3, 2000)
	pickle.dump(domains, open('domains_dump.pickle', 'wb'))
	pickle.dump(links, open('links_dump.pickle', 'wb'))









# - - - - -   MERDA VECCHIA   - - - - - - -

'''
def multiprocess_extract(links: list, level: int, workers=20):
	""" Extract unique domains recursively with multiple workers """
	levels = itertools.repeat(level)
	with multiprocessing.Pool(workers) as pool:
		result = pool.starmap(recursive_extract, zip(links, levels))

	# merge list of links generated by each worker
	all_links = reduce(lambda x,y: x+y, result)

	return extract_domains(all_links), all_links
'''


'''
def recursive_extract(url: str, level: int, verbose=True) -> set:
	""" Returns the set of unique domains starting of "url" and going 'level' recursive levels """

	if level <= 0:
		return [url]

	if verbose:
		print('>>> Crawling link <<<', url)

	links = extract_links(url)

	result = []
	for link in links:
		result += recursive_extract(link, level-1, verbose=True)

	return result
'''


